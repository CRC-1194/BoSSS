This section covers basic performance tests, i.e. how specific algorithms scale
with grid resolution or with polynomial degree, on a \emph{single compute node}.

% --------------------------------------------------------------------------------
\section{Solver Performance - Poisson problems}
\label{sec:SolverPerformancePoisson}
% --------------------------------------------------------------------------------
Three different groups of solvers are compared:
\begin{itemize}
\item
Direct Solvers: directs sparse methods, such as PARDISO\footnote{
\url{http://www.pardiso-project.org/}}
and MUMPS\footnote{
\url{http://mumps.enseeiht.fr/}}
are compared.
Their performance also serves as a comparative baseline.

\item
Iterative Algorithms without preconditioning, resp. low-impact, generic preconditioning:
This includes solver libraries such as \code{monkey} (BoSSS-specific, supports GPU)
as well as
HYPRE\footnote{
\url{https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods}}
(native library, used via wrappers).

\item
Iterative Algorithms with \ac{dg}-specific preconditioners, such as aggregation multigrid
and multi-level additive Schwarz
\end{itemize}

\subsection{Constant Diffusion Coefficient test problem}
\label{sec:ConstantDiffusionCoefficient}
The problem
\begin{equation}
\left\{ \begin{array} {rclll}
- \Delta T   & = & g_{\domain}                      
& \text{in}\ \Omega = (0,10) \times (-1,1) \times (-1,1)  &  \\
% ----
T   & = & g_D = 0                             
& \text{on}\ \Gamma_D = \{ (x,y,z) \in \real^3; \ x = 0 \}
& \text{Dirichlet-boundary} \\
% ----
\nabla T \cdot \vec{n}_{\partial \domain} & = & g_N 
& \text{on}\ \Gamma_N = \partial \Omega \setminus \Gamma_D
& \text{Neumann-boundary}
\end{array} \right.
\label{eq:ContantCoeffPoissonBenchmark}
\end{equation}
is investigated on a non-uniform, Cartesian grid
(equidistant in $z$, sinus-spacing in $x$ and $y$ direction).
The large $\Gamma_N$ makes the problem harder for non-preconditioned
iterative methods. See Figure \ref{fig:ConstantCoeffRuntimes} for results.

\graphicspath{{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/}}

Subsequent the performance of some code parts are investigated for the block Jacobian and the Schwarz multigrid preconditioned conjugate gradient method (PCG):
\begin{itemize}
	\item MatrixAssembly: assemble Block matrix
	\item Aggregation basis init: create multigrid sequence, contains information about the transformation at the multigrid levels
	\item Solver Init: hand over/assemble relevant data for the chosen solver, e.g. operator matrix etc.
	\item Solver Run: solves the equation system: operator matrix, vector of dg coordinates and given RHS 
\end{itemize}
Both variants are using pre and post smoothing solvers (5 runs at each multigrid level).



\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:ConstantCoeffRuntimes}
\end{figure}

\newpage

\subsubsection{Block Jacobian and Schwarz-Multigrid Preconditioning}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/zeug/jacobischwarz-scheme.pdf_tex}
	\end{center}
	\caption{
		schematic of the Multigrid Schwarz \& Block Jacobian Preconditioning (for a CG solver).
	}
	\label{fig:schema_BlockPCG}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoisson_MG.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_blockJacobianPCG}
\end{figure}
\newpage

\subsubsection{Schwarz-Multigrid Preconditioning}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/zeug/schwarz-scheme.pdf_tex}
	\end{center}
	\caption{
		schematic of the Multigrid Schwarz \& Block Jacobian Preconditioning (for a CG solver).
	}
	\label{fig:schema_SchwarzPCG}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoisson_Schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the Schwarz PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_SchwarzPGC}
\end{figure}
\newpage

\subsection{velocity test problem with levelset and Xdg}
\label{sec:XdgPoisson}
The problem
\begin{equation}
\left\{ \begin{array} {rclll}
- \nabla \cdot (\mu \nabla \vec{u})   & = & g_{\domain}=(1, 1, 1)^T                       
& \text{in}\ \Omega \setminus \varphi, \text{with}\ \Omega= [-1,1]^3  &  \\
% ----
\vec{u_D} & = & (0, 0, 0)^T                             
& \text{on}\ \Gamma_D = \partial \Omega
& \text{Dirichlet-boundary} \\
% ----
\jump{\mu \nabla \vec{u}} \cdot \vec{n} & = & (0, 0, 0)^T  & \text{on}\ \varphi = \{\vec{X}=(x,y,z); \ |\vec{X}|_2=0.7\} & \text{levelset}
\end{array} \right.
\label{eq:XdgPoissonBenchmark}
\end{equation}
where $\mu_1=1$ (inner) and $\mu_2=1000$ (outer) characterize the two phases. is investigated on a uniform, equidistant Cartesian grid. See \ref{fig:XdgRuntimes} for results.

\graphicspath{{./apdx-NodeSolverPerformance/XDGPoisson/plots/}}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoissonScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:XdgRuntimes}
\end{figure}
\newpage
Subsequent the performance of some code parts are investigated for the block Jacobian and the Schwarz multigrid preconditioned conjugate gradient method (PCG):
\begin{itemize}
	\item MatrixAssembly: assemble Block matrix
	\item Aggregation basis init: create multigrid sequence, contains information about the transformation at the multigrid levels
	\item Solver Init: hand over/assemble relevant data for the chosen solver, e.g. operator matrix etc.
	\item Solver Run: solves the equation system: operator matrix, vector of dg coordinates and given RHS 
\end{itemize}
Both variants are using pre and post smoothing solvers (5 runs at each multigrid level).
\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoisson_MG.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_blockJacobianPCG}
\end{figure}
\newpage
\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XdgPoisson/plots/XdgPoisson_Schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the Schwarz PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_SchwarzPGC}
\end{figure}
\newpage
\section{Solver Performance - Navier-Stokes problems}
\label{sec:SolverPerformanceNSE}
Different solver strategies are conducted to solve the fully coupled incompressible Navier-Stokes equations. At the moment the following strategies can be examined:
\begin{itemize}
	\item Linearizsation of the NSE with: Newton(Gmres) or Picard
	\item Solving the linear problem with a Gmres approach or the direct solver MUMPS
	\item Preconditioning with Additive-Schwarz domain decomposition (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks (Automatic)
	\item Preconditioning with Additive-Schwarz kcycle Blocks on the coarsest multigrid level (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks
\end{itemize}
\subsection{Driven Cavity 3D}
The problem
\begin{equation}
\left\{ \begin{array} {rclll}
\rho_f\Big(\frac{\partial \vec{u}}{\partial t}+ \vec{u} \cdot \nabla \vec{u}\Big) +\nabla p - \mu_f \Delta \vec{u} & = & \vec{f}
& \text{and}\   &  \\
% ----
\nabla \cdot \vec{u} & = & 0
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)  & \\
\vec{u}_D & = & \{1,0,0 \}
& \text{on}\ \Gamma_D = \{ (x,y,0z) \in \real^3; \ z = 0.5 \}
& \text{Dirichlet-boundary}\\
\vec{u}_W & = & 0
& \text{on}\ \Gamma_W = \partial \Omega \setminus \Gamma_D
& \text{Dirichlet-boundary}\\
\vec{u}_0(x,y,z) & = & \{1,0,0\}
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)
& \text{Initial Condition}
\end{array} \right.
\label{eq:NavierStokesCavityBenchmark}
\end{equation}
is investigated on different cartesian grids. The physical parameters of the fluid are choosen to be $\rho_f=1$ and $\mu_f=0.0025$ which renders down to a Reynoldsnumber of 400.

\graphicspath{{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/}}

\begin{figure}[h!]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/NodePerformance.tex}
	\end{center}
	\caption{
		Solver runtime vs. DoFs, for polynomial degree $k=2/1$,
		for problem/Equation (\ref{eq:NavierStokesCavityBenchmark}).
	}
	\label{fig:DrivenCavity}
\end{figure}