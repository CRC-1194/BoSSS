This section covers basic performance tests, i.e. how specific algorithms scale
with grid resolution and with polynomial degree, on a \emph{single compute node}.




% --------------------------------------------------------------------------------
\section{Solver Performance - Poisson/Stokes problems}
\label{sec:SolverPerformancePoisson}
% --------------------------------------------------------------------------------
Two groups of solver are compared:
\begin{itemize}
\item
Direct Solvers: directs sparse methods, such as PARDISO\footnote{
\url{http://www.pardiso-project.org/}}
and MUMPS\footnote{
\url{http://mumps.enseeiht.fr/}}
are compared.
Their performance also serves as a comparative baseline.

%\item
%Iterative Algorithms without preconditioning, resp. low-impact, generic preconditioning:
%This includes solver libraries such as \code{monkey} (BoSSS-specific, supports GPU)
%as well as
%HYPRE\footnote{
%\url{https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods}}
%(native library, used via wrappers).

\item
Iterative Algorithms with \ac{dg}-specific preconditioners, such as aggregation multigrid
and multi-level additive Schwarz
\end{itemize}

The scaling and performance profile is investigated in subsequent sections. For Performance profiling we stick to our working horse: the kcycle-Schwarz algorithm.
The performance profile of the krylov V-cycle with Schwarz pre and post-smoother is investigated. A direct solver (PARDISO) is used to solve the deepest coarse system. One may choose another direct solver for the coarse system, e.g. MUMPS. In practise PARDISO is more robust to ill-conditioned system, therefore in this performance analysis investigation we will stick to PARDISO as solver, wherever a direct solver is needed.
NOTE: the p-two-grid used in Schwarz or as a standalone preconditioner, the coarse system is solved by a direct solver.

We distinguish four phases of every solver-scenario: 
\begin{itemize}
	\item MatrixAssembly: assemble Block matrix
	\item Aggregation basis init: create multigrid sequence, contains information about the transformation at the multigrid levels
	\item Solver Init: hand over/assemble relevant data for the chosen solver, e.g. operator matrix etc.
	\item Solver Run: solves the equation system: operator matrix, vector of dg coordinates and given RHS 
\end{itemize}
Matrix assembly and aggregation init is discritization specific, whereas, Solver init and run ist specific for the used solver.

\subsection{Introduction of solvers}

\subsubsection{linear Solver: preconditioned GMRES}
\label{sec:ptg_gmres}



\newcommand{\gvec}[1]{\boldsymbol{#1}}
%%\newcommand{\gmat}[1]{\underline{\underline{#1}}}
\newcommand{\gmat}[1]{\boldsymbol{\mathrm{#1}}}
%\newcommand{\setI}{\mathbb{I}}
%\newcommand{\setV}{\mathbb{V}}

The pTG algorithm, used as left preconditioner of the well known GMRES-algorithm ...

... TBD: Darstellung klÃ¤ren ...
%\begin{algorithmic}
%	\label{alg:pMultigrid}
%	\Require A right-hand-side $\gvec{b}$,
%	a vector of DG polynomial degrees $\gvec{k}_{\text{lo}}$, 
%	which separates low from high order modes, for each variable.
%	Optionally, an index-set $\mathbb{I} \subset \mathbb{J}^\lambda$ of 
%	sub-cells, denoting the block to solve.
%	\Ensure  An approximate solution $\gvec{x}$
%	\State
%	$\gvec{x} := 0$ 
%	\Comment{initialize approximate solution}
%	\State $I_{\text{lo}} := m(\mathbb{I},-,-,\leq N_{\gvec{k}_{\text{lo}}})$
%	\Comment{Indices for low-order modes in cells $\mathbb{I}$}
%	\State 
%	$\gmat{M}_{\text{lo}} := \gmat{M}_{ I_{\text{lo}} , I_{\text{lo}} }$
%	\Comment{extract low-order system}
%	\State 
%	$\gvec{r}_{\text{lo}} := \gvec{b}_{ I_{\text{lo}} }$
%	\Comment{extract low-order RHS}
%	\State Solve: $\gvec{x}_{\text{lo}} := \gmat{M}_{\text{lo}} \setminus \gvec{r}_{\text{lo}}$
%	\Comment{using a sparse direct solver}
%	\State
%	$\gvec{x}_{ I_{\text{lo}} } := \gvec{x}_{ I_{\text{lo}} } + \gvec{x}_{\text{lo}}$
%	\Comment{accumulate low-order solution}
%	\State
%	$\gvec{r} := \gvec{b} - \gmat{M}\gvec{x}_{ I_{\text{lo}} }$
%	\Comment{compute residual of solution}
%	\ForAll {$j \in \mathbb{I}$}
%	\Comment{loop over cells...}
%	\State $I_{\text{hi},j} := m(j,-,-,> N_{\gvec{k}_{\text{lo}}})$
%	\Comment{Indices for high-order modes in cell $j$}
%	\State $\gmat{M}_{\text{hi},j} := \gmat{M}_{I_{\text{hi},j} \ I_{\text{hi},j}}$
%	\Comment{extract high-order system in cell $j$}
%	\State $\gvec{r}_{\text{hi},j} := \gvec{r}_{ I_{\text{hi},j} }$
%	\Comment{extract high-order residuum in cell $j$}
%	\State Solve: $\gvec{x}_{\text{hi},j} := \gmat{M}_{\text{hi},j} \setminus \gvec{r}_{\text{hi},j}$
%	\Comment{using a dense direct solver}
%	\State $\gvec{x}_{ I_{\text{hi},j} } := \gvec{x}_{ I_{\text{hi},j} } + \gvec{x}_{\text{hi},j}$
%	\Comment{accumulate local high-order correction}
%	\EndFor
%\end{algorithmic}


\subsubsection{linear Solver: V-krylov-cycle with Schwarz smoother}
\label{sec:kcycle}

The V-krylov-cycle with Schwarz smoother with pTG, which is introduced in \ref{sec:ptg_gmres}, as block solver.

... TBD: Darstellung klÃ¤ren ...
%\begin{algorithmic}
%	\label{alg:OrthoMG}
%	\Require A right-hand-side $\gvec{b}$ and an initial solution guess $\gvec{x}_0$ (can be zero).
%	\Ensure An approximate solution $\gvec{x}$ whose residual norm is 
%	less than or equal to the residual norm of the initial guess $\gvec{x}_0$. 
%	\State $\gmat{Z} := (), \gmat{W} := ()$
%	\Comment{Initialize as empty}
%	\State $\gvec{r}_0 := \gvec{b} - \gmat{M} \gvec{x}_0$, $\gvec{r} := \gvec{r}_0$, $\gvec{x} := \gvec{x}_0$
%	\Comment{residual of interstitial solution}
%	\While{$|\gvec{r}|_2 > \varepsilon$}
%	\State{ $\gvec{z} := \text{Swz}(\gvec{r})$}
%	\Comment{pre-smoother}
%	\State{$(\gvec{x}, \gvec{r}) = \text{RM}(\gvec{x}_0, \gvec{z}, \gmat{W}, \gmat{Z})$}
%	\Comment{minimize residual of pre-smoother}
%	\State{$\gvec{r}_c := (\gmat{R}^{X,\lambda})^T \gvec{r}$}
%	\Comment{restrict residual}
%	\If {\text{ dimension of matrix } $L_{c}>L_{min}$ }  %%$\lambda + 1 < \lambda_{max}$
%	\State{$(\gvec{z}_c, \gvec{r}_c) = \text{MG}(0,\gvec{r}_c, \gmat{M}^{\lambda + 1})$}
%	\Comment{call multigrid on coarser level}
%	\Else
%	\State  $\gvec{z}_c := \gmat{M}_{c} \setminus \gvec{r}_{c}$
%	\Comment{using a sparse direct solver}
%	\EndIf
%	\State{$\gvec{z} := \gmat{R}^{X,\lambda} \gvec{z}_c $}
%	\Comment{prolongate coarse-grid correction}
%	\State{$(\gvec{x}, \gvec{r}) = \text{RM}(\gvec{x}_0, \gvec{z}, \gmat{W}, \gmat{Z})$}
%	\Comment{minimize residual}
%	\State{ $\gvec{z} := \text{Swz}(\gvec{r})$}
%	\Comment{pre-smoother}
%	\State{$(\gvec{x}, \gvec{r}) = \text{RM}(\gvec{x}_0, \gvec{z}, \gmat{W}, \gmat{Z})$}
%	\Comment{minimize residual of pre-smoother}
%	\EndWhile
%\end{algorithmic}


\subsection{DG-Poisson test problem}
\label{sec:ConstantDiffusionCoefficient}
The stationary 3D-problem:
\begin{equation}
\left\{ \begin{array} {rclll}
- \Delta T   & = & g_{\domain}                      
& \text{in}\ \Omega = (0,10) \times (-1,1) \times (-1,1)  &  \\
% ----
T   & = & g_D = 0                             
& \text{on}\ \Gamma_D = \{ (x,y,z) \in \real^3; \ x = 0 \}
& \text{Dirichlet-boundary} \\
% ----
\nabla T \cdot \vec{n}_{\partial \domain} & = & g_N 
& \text{on}\ \Gamma_N = \partial \Omega \setminus \Gamma_D
& \text{Neumann-boundary}
\end{array} \right.
\label{eq:ContantCoeffPoissonBenchmark}
\end{equation}
where $g_{\domain}=-sin(x)$.
is investigated on a non-uniform, Cartesian grid
(equidistant in $z$, sinus-spacing in $x$ and $y$ direction).
The large $\Gamma_N$ makes the problem harder for non-preconditioned
iterative methods. See Figure \ref{fig:ConstantCoeffRuntimes} for results.

\subsection{DG-Poisson: scaling of solvers}

\graphicspath{{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/}}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonScaling.tex}
	\end{center}
	\caption{
		Solver wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:ConstantCoeffRuntimes}
\end{figure}

\newpage

\subsubsection{DG-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the V-kcycle with additive Schwarz (p-two-grid as block solver) smoother. wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_blockJacobianPCG}
\end{figure}
\newpage

\subsubsection{DG-Poisson: preconditioned GMRES Profiling}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonexp_gmres_levelpmg.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the preconditioned GMRES algorithm. Wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_SchwarzPGC}
\end{figure}
\newpage

\subsection{Xdg-Poisson test problem}
\label{sec:XdgPoisson}

\newcommand{\frakA}{\mathfrak{A}}
\newcommand{\frakB}{\mathfrak{B}}
\newcommand{\nOmega}{\vec{n}_{\partial \Omega } }
%\newcommand*{\jump}[1]{\left\llbracket {#1} \right\rrbracket}
\newcommand{\frakI}{\mathfrak{I}}
\newcommand{\nI}{\vec{n}_\frakI}

The test problem can be considered as stationary 3 dimensional heat equation with source-term and with two phases:
\begin{equation}
\left\{ \begin{array}{rll}
- \mu \Delta u                   & = f               & \text{ in } \Omega \setminus \frakI , \\
\jump{u}                         & = 0               & \text{ on } \frakI ,                  \\
\jump{\mu \nabla u \cdot \nI}    & = 0               & \text{ on } \frakI ,                  \\
u                                & = g_\text{Diri}   & \text{ on } \Gamma_\mathrm{Diri} ,    \\
\nabla u \cdot \nOmega           & = g_\text{Neu}    & \text{ on } \Gamma_\mathrm{Neu} .     \\
\end{array}
\right.
\label{eq:poisson-jump-problem-def}
\end{equation}
with a constant diffusion coefficient in each subdomain


\begin{equation}
\mu (\vec{x}) = 
\left\{ \begin{array}{ll}
\mu_\frakA & \text{for } \vec{x} \in \frakA, \\
\mu_\frakB & \text{for } \vec{x} \in \frakB. \\
\end{array} \right.
\label{eq:DiscDiffKoeff}
\end{equation}

where $\mu_1=1$ (inner) and $\mu_2=1000$ (outer) characterize the two phases. is investigated on a uniform, equidistant Cartesian grid. See \ref{fig:XdgRuntimes} for results.

\graphicspath{{./apdx-NodeSolverPerformance/XDGPoisson/plots/}}

\subsubsection{Xdg-Poisson: scaling of solvers}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoissonScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:XdgRuntimes}
\end{figure}
\newpage

\subsubsection{Xdg-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoissonexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_blockJacobianPCG}
\end{figure}

\subsubsection{Xdg-Poisson: preconditioned GMRES Profiling}


\newpage
\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XdgPoisson/plots/XdgPoissonexp_gmres_levelpmg.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the Schwarz PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_SchwarzPGC}
\end{figure}
\newpage

\subsection{Xdg-Stokes test problem}

As a test case for a two-phase stokes problem with Xdg approach, we choose an ellipsoid within a closed cube. The body is not touching the cube. There is no gravitational force and the boundaries are considered as walls ($\vec{u}_\mathrm{Diri}=\vec{0}$).

\newcommand{\divergence}[1]{{\mathrm{div}\left({#1}\right)}}
\newcommand{\normI}{{\vec{n}_{\frakI}}}

\begin{equation}
\left\{ \begin{array}{rll}

\nabla p
- 
\divergence{\mu ( \nabla \vec{u} + (   \nabla \vec{u})^T ) }  
& = 0  
& \text{ in } \Omega \setminus \frakI = (-1,1)^3 , \\
\textrm{div}(\vec{u})  &=  0    
& \text{ in } \Omega \setminus \frakI , \\
\jump{\vec{u}} & =  0
& \text{ on } \frakI \\ 
\jump{
	p \nI
	- \mu ( \nabla \vec{u} + (   \nabla \vec{u})^T ) \cdot \nI
}
& = 
\sigma \kappa \normI
& \text{ on } \frakI(t), \\ 
u  & = \vec{u}_\mathrm{Diri}   
& \text{ on } \Gamma_\mathrm{Diri} .    \\
\end{array} \right.
\label{eq:XdgStokes-Benchmark}
\end{equation}
with piece-wise constant density and viscosity for both phases, i.e.
\begin{equation}
\rho(\vec{x}) = \left\{ \begin{array}{ll}
\rho_\frakA & \textrm{for } \vec{x} \in \frakA \\
\rho_\frakB & \textrm{for } \vec{x} \in \frakB \\
\end{array} \right.
\quad \textrm{and} \quad
\mu(\vec{x}) = \left\{ \begin{array}{ll}
\mu_\frakA & \textrm{for } \vec{x} \in \frakA \\
\mu_\frakB & \textrm{for } \vec{x} \in \frakB \\
\end{array} \right.
.
\label{eq:defRhoAndMu}
\end{equation}
Furthermore, $\sigma$ denotes surface tension and $\kappa$ denotes the 
mean curvature of $\frakI$. The body (ellipsoid) is defined by a level-set function:

\begin{equation}
(x/(\beta*r))^2 + (y/r)^2 +(z/r)^2-1=0
\end{equation} 

where $\beta=0.5$ is the spherical aberration and $r=0.5$ the radius. The physical parameters are:

\begin{table}[h]
	\centering
	\begin{tabular}{l|c}
		$\rho_A$ & 1e-3  $kg / cm^3$\\
		$\rho_B$ & 1.2e-6  $kg / cm^3$\\
		$\mu_A$ & 1e-5 $kg / cm / sec$\\
		$\mu_B$ & 17.1e-8 $kg / cm / sec$\\
		$\sigma$ & 72.75e-3 $kg / sec^2$\\
	\end{tabular}
\end{table}

The surface tension is inducing a velocity field around the ellipsoid. This test case is non-physical due to the static body. A more realistic body would reshape to compensate the surface tension, which leads to oscillation of the body.

\graphicspath{{./apdx-NodeSolverPerformance/XDGStokes/plots/}}

\subsubsection{Xdg-Poisson: scaling of solvers}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGStokes/plots/XdgStokesScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgStokes-Benchmark}).
	}
	\label{fig:XdgStokes-scaling}
\end{figure}
\newpage

\subsubsection{Xdg-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGStokes/plots/XdgStokesexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgStokes-Benchmark}).
	}
	\label{fig:XdgStokes-kcylce}
\end{figure}


\section{Solver Performance - Navier-Stokes problems}
\label{sec:SolverPerformanceNSE}
Different solver strategies are conducted to solve the fully coupled incompressible Navier-Stokes equations. At the moment the following strategies can be examined:
\begin{itemize}
	\item Linearizsation of the NSE with: Newton(Gmres) or Picard
	\item Solving the linear problem with a Gmres approach or the direct solver MUMPS
	\item Preconditioning with Additive-Schwarz domain decomposition (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks (Automatic)
	\item Preconditioning with Additive-Schwarz kcycle Blocks on the coarsest multigrid level (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks
\end{itemize}
\subsection{Driven Cavity 3D}
The problem
\begin{equation}
\left\{ \begin{array} {rclll}
\rho_f\Big(\frac{\partial \vec{u}}{\partial t}+ \vec{u} \cdot \nabla \vec{u}\Big) +\nabla p - \mu_f \Delta \vec{u} & = & \vec{f}
& \text{and}\   &  \\
% ----
\nabla \cdot \vec{u} & = & 0
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)  & \\
\vec{u}_D & = & \{1,0,0 \}
& \text{on}\ \Gamma_D = \{ (x,y,0z) \in \real^3; \ z = 0.5 \}
& \text{Dirichlet-boundary}\\
\vec{u}_W & = & 0
& \text{on}\ \Gamma_W = \partial \Omega \setminus \Gamma_D
& \text{Dirichlet-boundary}\\
\vec{u}_0(x,y,z) & = & \{1,0,0\}
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)
& \text{Initial Condition}
\end{array} \right.
\label{eq:NavierStokesCavityBenchmark}
\end{equation}
is investigated on different cartesian grids. The physical parameters of the fluid are choosen to be $\rho_f=1$ and $\mu_f=0.0025$ which renders down to a Reynoldsnumber of 400.

\graphicspath{{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/}}

\begin{figure}[h!]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/NodePerformance.tex}
	\end{center}
	\caption{
		Solver runtime vs. DoFs, for polynomial degree $k=2/1$,
		for problem/Equation (\ref{eq:NavierStokesCavityBenchmark}).
	}
	\label{fig:DrivenCavity}
\end{figure}