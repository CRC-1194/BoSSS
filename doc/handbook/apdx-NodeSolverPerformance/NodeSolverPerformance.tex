This section covers basic performance tests, i.e. how specific algorithms scale
with grid resolution and with polynomial degree, on a \emph{single compute node}.

% --------------------------------------------------------------------------------
\section{Solver Performance - Poisson/Stokes problems}
\label{sec:SolverPerformancePoisson}
% --------------------------------------------------------------------------------
Two groups of solver are compared:
\begin{itemize}
\item
Direct Solvers: directs sparse methods, such as PARDISO\footnote{
\url{http://www.pardiso-project.org/}}
and MUMPS\footnote{
\url{http://mumps.enseeiht.fr/}}
are compared.
Their performance also serves as a comparative baseline.

%\item
%Iterative Algorithms without preconditioning, resp. low-impact, generic preconditioning:
%This includes solver libraries such as \code{monkey} (BoSSS-specific, supports GPU)
%as well as
%HYPRE\footnote{
%\url{https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods}}
%(native library, used via wrappers).

\item
Iterative Algorithms with \ac{dg}-specific preconditioners, such as aggregation multigrid
and multi-level additive Schwarz
\end{itemize}

The scaling and performance is profiled subsequent sections. For Performance profiling we stick to our working horse: the kcycle-Schwarz algorithm (with optional p-two-grid as block solver).
The performance profile of the krylov V-cycle with Schwarz pre and post-smoother is investigated. A direct solver (PARDISO) is used to solve the deepest coarse system. One may choose another direct solver for the coarse system, e.g. MUMPS. In practise PARDISO is more robust to ill-conditioned system, therefore in this performance analysis investigation we will stick to PARDISO as solver, wherever a direct solver is needed.
NOTE: the p-two-grid used in Schwarz or as a standalone preconditioner, the coarse system is solved by a direct solver.

We distinguish four phases of every solver-scenario: 
\begin{itemize}
	\item MatrixAssembly: assemble Block matrix
	\item Aggregation basis init: create multigrid sequence, contains information about the transformation at the multigrid levels
	\item Solver Init: hand over/assemble relevant data for the chosen solver, e.g. operator matrix etc.
	\item Solver Run: solves the equation system: operator matrix, vector of dg coordinates and given RHS 
\end{itemize}
Matrix assembly and aggregation init is discritization specific, whereas, Solver init and run ist specific for the used solver.

\subsection{Introduction of solvers}

\subsubsection{linear Solver: p-two-grid}
\label{sec:ptg_gmres}

The p two grid algorithm can be used as left preconditioner for the well known GMRES-algorithm. Or as block solver of the Schwarz blocks in the Orthonormalization-multigrid algorithm, described in \ref{alg:OrthoMG}.

\subsubsection{linear Solver: V-krylov-cycle with Schwarz smoother}
\label{sec:kcycle}

The orthonormalization multigrid is a combination of a v-cycle of a geometric multigrid (or algebraic as the agglomeration of cells is graph based) with an additive Schwarz smoother and a projection method onto the history of residual contributions of parts of the algorithm (smoother and coarse grid correction). A schematic of the solver can be checked out in \ref{fig:SolverScheme}. 
For more details on the solvers check \cite{OpenSoftwarePDE}.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.65]{./apdx-NodeSolverPerformance/solver-scheme.png}
		\input{./apdx-NodeSolverPerformance/solver-scheme.png}
	\end{center}
	\caption{
		scheme of Orthonormalization Multigrid. The block solver is usually a direct solver like PARDISO. Note: for the smoother it is sufficient to solve the system approximately. This gives rise to approximate solutions of the Schwarz blocks, like a ILU algorithm.  
	}
	\label{fig:SolverScheme}
\end{figure}

\subsection{DG-Poisson test problem}
\label{sec:ConstantDiffusionCoefficient}
The stationary 3D-problem:
\begin{equation}
\left\{ \begin{array} {rclll}
- \Delta T   & = & g_{\domain}                      
& \text{in}\ \Omega = (0,10) \times (-1,1) \times (-1,1)  &  \\
% ----
T   & = & g_D = 0                             
& \text{on}\ \Gamma_D = \{ (x,y,z) \in \real^3; \ x = 0 \}
& \text{Dirichlet-boundary} \\
% ----
\nabla T \cdot \vec{n}_{\partial \domain} & = & g_N 
& \text{on}\ \Gamma_N = \partial \Omega \setminus \Gamma_D
& \text{Neumann-boundary}
\end{array} \right.
\label{eq:ContantCoeffPoissonBenchmark}
\end{equation}
where $g_{\domain}=-sin(x)$.
is investigated on a non-uniform, Cartesian grid
(equidistant in $z$, sinus-spacing in $x$ and $y$ direction).
The large $\Gamma_N$ makes the problem harder for non-preconditioned
iterative methods. See Figure \ref{fig:ConstantCoeffRuntimes} for results.

\subsection{DG-Poisson: scaling of solvers}

\graphicspath{{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/}}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonScaling.tex}
	\end{center}
	\caption{
		Solver wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:ConstantCoeffRuntimes}
\end{figure}

\newpage

\subsubsection{DG-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the V-kcycle with additive Schwarz (p-two-grid as block solver) smoother. wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_blockJacobianPCG}
\end{figure}
\newpage

\subsubsection{DG-Poisson: preconditioned GMRES Profiling}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/PoissonConstCoeff/plots/ConstCoeffPoissonexp_gmres_levelpmg.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the preconditioned GMRES algorithm. Wallclock-time vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:ContantCoeffPoissonBenchmark}).
	}
	\label{fig:SIP_SchwarzPGC}
\end{figure}
\newpage

\subsection{Xdg-Poisson test problem}
\label{sec:XdgPoisson}

\newcommand{\frakA}{\mathfrak{A}}
\newcommand{\frakB}{\mathfrak{B}}
\newcommand{\nOmega}{\vec{n}_{\partial \Omega } }
%\newcommand*{\jump}[1]{\left\llbracket {#1} \right\rrbracket}
\newcommand{\frakI}{\mathfrak{I}}
\newcommand{\nI}{\vec{n}_\frakI}

The test problem can be considered as stationary 3 dimensional heat equation with source-term and with two phases:
\begin{equation}
\left\{ \begin{array}{rll}
- \mu \Delta u                   & = f               & \text{ in } \Omega \setminus \frakI , \\
\jump{u}                         & = 0               & \text{ on } \frakI ,                  \\
\jump{\mu \nabla u \cdot \nI}    & = 0               & \text{ on } \frakI ,                  \\
u                                & = g_\text{Diri}   & \text{ on } \Gamma_\mathrm{Diri} ,    \\
\nabla u \cdot \nOmega           & = g_\text{Neu}    & \text{ on } \Gamma_\mathrm{Neu} .     \\
\end{array}
\right.
\label{eq:XdgPoissonBenchmark}
\end{equation}
with a constant diffusion coefficient in each subdomain


\begin{equation}
\mu (\vec{x}) = 
\left\{ \begin{array}{ll}
\mu_\frakA & \text{for } \vec{x} \in \frakA, \\
\mu_\frakB & \text{for } \vec{x} \in \frakB. \\
\end{array} \right.
\label{eq:DiscDiffKoeff}
\end{equation}

where $\mu_1=1$ (inner) and $\mu_2=1000$ (outer) characterize the two phases. is investigated on a uniform, equidistant Cartesian grid. See \ref{fig:XdgRuntimes} for results.

\graphicspath{{./apdx-NodeSolverPerformance/XDGPoisson/plots/}}

\subsubsection{Xdg-Poisson: scaling of solvers}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoissonScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:XdgRuntimes}
\end{figure}
\newpage

\subsubsection{Xdg-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGPoisson/plots/XdgPoissonexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_blockJacobianPCG}
\end{figure}

\subsubsection{Xdg-Poisson: preconditioned GMRES Profiling}


\newpage
\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XdgPoisson/plots/XdgPoissonexp_gmres_levelpmg.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the Schwarz PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgPoissonBenchmark}).
	}
	\label{fig:Xdg_SchwarzPGC}
\end{figure}
\newpage

\subsection{Xdg-Stokes test problem}

As a test case for a two-phase stokes problem with Xdg approach, we choose an ellipsoid within a closed cube. The body is not touching the cube. There is no gravitational force and the boundaries are considered as walls ($\vec{u}_\mathrm{Diri}=\vec{0}$).

\newcommand{\divergence}[1]{{\mathrm{div}\left({#1}\right)}}
\newcommand{\normI}{{\vec{n}_{\frakI}}}

\begin{equation}
\left\{ \begin{array}{rll}

\nabla p
- 
\divergence{\mu ( \nabla \vec{u} + (   \nabla \vec{u})^T ) }  
& = 0  
& \text{ in } \Omega \setminus \frakI = (-1,1)^3 , \\
\textrm{div}(\vec{u})  &=  0    
& \text{ in } \Omega \setminus \frakI , \\
\jump{\vec{u}} & =  0
& \text{ on } \frakI \\ 
\jump{
	p \nI
	- \mu ( \nabla \vec{u} + (   \nabla \vec{u})^T ) \cdot \nI
}
& = 
\sigma \kappa \normI
& \text{ on } \frakI(t), \\ 
u  & = \vec{u}_\mathrm{Diri}   
& \text{ on } \Gamma_\mathrm{Diri} .    \\
\end{array} \right.
\label{eq:XdgStokes-Benchmark}
\end{equation}
with piece-wise constant density and viscosity for both phases, i.e.
\begin{equation}
\rho(\vec{x}) = \left\{ \begin{array}{ll}
\rho_\frakA & \textrm{for } \vec{x} \in \frakA \\
\rho_\frakB & \textrm{for } \vec{x} \in \frakB \\
\end{array} \right.
\quad \textrm{and} \quad
\mu(\vec{x}) = \left\{ \begin{array}{ll}
\mu_\frakA & \textrm{for } \vec{x} \in \frakA \\
\mu_\frakB & \textrm{for } \vec{x} \in \frakB \\
\end{array} \right.
.
\label{eq:defRhoAndMu}
\end{equation}
Furthermore, $\sigma$ denotes surface tension and $\kappa$ denotes the 
mean curvature of $\frakI$. The body (ellipsoid) is defined by a level-set function:

\begin{equation}
(x/(\beta*r))^2 + (y/r)^2 +(z/r)^2-1=0
\end{equation} 

where $\beta=0.5$ is the spherical aberration and $r=0.5$ the radius. The physical parameters are:

\begin{table}[h]
	\centering
	\begin{tabular}{l|c}
		$\rho_A$ & 1e-3  $kg / cm^3$\\
		$\rho_B$ & 1.2e-6  $kg / cm^3$\\
		$\mu_A$ & 1e-5 $kg / cm / sec$\\
		$\mu_B$ & 17.1e-8 $kg / cm / sec$\\
		$\sigma$ & 72.75e-3 $kg / sec^2$\\
	\end{tabular}
\end{table}

The surface tension is inducing a velocity field around the ellipsoid. This test case is non-physical due to the static body. A more realistic body would reshape to compensate the surface tension, which leads to oscillation of the body.

\graphicspath{{./apdx-NodeSolverPerformance/XDGStokes/plots/}}

\subsubsection{Xdg-Poisson: scaling of solvers}

\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGStokes/plots/XdgStokesScaling.tex}
	\end{center}
	\caption{
		Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgStokes-Benchmark}).
	}
	\label{fig:XdgStokes-scaling}
\end{figure}

The size of Schwarzblocks was set to 10.000 DOF. It is known, that this raises the number of iterations and therefore the number of Schwarz blocks shall be constant for next study ... 

\newpage

\subsubsection{Xdg-Poisson: krylov-cycle Profiling}


\begin{figure}[!h]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/XDGStokes/plots/XdgStokesexp_Kcycle_schwarz.tex}
	\end{center}
	\caption{
		Investigation of runtime of different code parts of the block Jacobian PCG. Solver runtime vs. degrees-of-freedom, for different polynomial degrees $k$,
		for problem/Equation (\ref{eq:XdgStokes-Benchmark}).
	}
	\label{fig:XdgStokes-kcylce}
\end{figure}

\cleardoublepage

\section{Solver Performance - Navier-Stokes problems}
\label{sec:SolverPerformanceNSE}
Different solver strategies are conducted to solve the fully coupled incompressible Navier-Stokes equations. At the moment the following strategies can be examined:
\begin{itemize}
	\item Linearizsation of the NSE with: Newton(Gmres) or Picard
	\item Solving the linear problem with a Gmres approach or the direct solver MUMPS
	\item Preconditioning with Additive-Schwarz domain decomposition (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks (Automatic)
	\item Preconditioning with Additive-Schwarz kcycle Blocks on the coarsest multigrid level (with coarse solve on the coarsest multigrid level) and direct solver MUMPS for the Blocks
\end{itemize}
\subsection{Driven Cavity 3D}
The problem
\begin{equation}
\left\{ \begin{array} {rclll}
\rho_f\Big(\frac{\partial \vec{u}}{\partial t}+ \vec{u} \cdot \nabla \vec{u}\Big) +\nabla p - \mu_f \Delta \vec{u} & = & \vec{f}
& \text{and}\   &  \\
% ----
\nabla \cdot \vec{u} & = & 0
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)  & \\
\vec{u}_D & = & \{1,0,0 \}
& \text{on}\ \Gamma_D = \{ (x,y,0z) \in \real^3; \ z = 0.5 \}
& \text{Dirichlet-boundary}\\
\vec{u}_W & = & 0
& \text{on}\ \Gamma_W = \partial \Omega \setminus \Gamma_D
& \text{Dirichlet-boundary}\\
\vec{u}_0(x,y,z) & = & \{1,0,0\}
& \text{in}\ \Omega = (-0.5,0.5) \times (-0.5,0.5) \times (-0.5,0.5)
& \text{Initial Condition}
\end{array} \right.
\label{eq:NavierStokesCavityBenchmark}
\end{equation}
is investigated on different cartesian grids. The physical parameters of the fluid are choosen to be $\rho_f=1$ and $\mu_f=0.0025$ which renders down to a Reynoldsnumber of 400.

\graphicspath{{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/}}

\begin{figure}[h!]
	\begin{center}
		\input{./apdx-NodeSolverPerformance/NavierStokesDrivenCavity/plots/NodePerformance.tex}
	\end{center}
	\caption{
		Solver runtime vs. DoFs, for polynomial degree $k=2/1$,
		for problem/Equation (\ref{eq:NavierStokesCavityBenchmark}).
	}
	\label{fig:DrivenCavity}
\end{figure}