\documentclass[a4paper,10pt]{report} % book, amsbook, amsproc ?
\input{common.tex}


% Title Page
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{subcaption} 
%\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pgfplots}


\newcommand{\cut}{\textrm{X}}
\newcommand{\grid}{\mathfrak{K}}
\newcommand{\gridCC}{\mathfrak{K}_{\textrm{CC}}}
\newcommand{\gridNear}{\mathfrak{K}_{\textrm{near}}}
\newcommand{\gridNarrow}{\mathfrak{K}_{\textrm{narrow}}}
\newcommand{\gridSolid}{\mathfrak{K}_{\textrm{S}}}
\newcommand{\gridFluid}{\mathfrak{K}_{\textrm{F}}}
\newcommand{\gridCCM}{\grid^\cut}
\newcommand{\gridAgg}[1]{\grid^{\cut, \alpha, {#1}}}
\renewcommand{\vec}[1]{\textbf{#1}}

\newcommand{\spaceVk}{\mathbb{V}_{\vec{k}}^{\textrm{X},\alpha}}
\newcommand{\Pdg}{\mathbb{P}_k(\grid,t)}
\newcommand{\Pcg}{\mathbb{P}_{\tilde{k}}(\grid,t)}
\newcommand{\PolySpace}[2]{\mathbb{P}^{#1}_{#2}}
\newcommand{\CutPolySpace}[1]{\PolySpace{\cut}{#1}}
\newcommand{\AggCutPolySpace}[1]{\PolySpace{\cut,\alpha}{#1}}
\newcommand{\Matrix}[1]{\textbf{#1}}
\newcommand{\Vector}[1]{\textbf{#1}}
\newcommand{\Def}{\textbf{Definition}}
\newcommand{\Interface}{\mathcal{J}}
\renewcommand{\cupdot}{\dot{\cup}}
\newcommand{\Eqref}[1]{Equation \ref{#1}}
\newcommand{\Figref}[1]{Figure \ref{#1}}
\newcommand{\Tabref}[1]{Table \ref{#1}}
\newcommand{\GammaCC}{\Gamma_{CC}}
\newcommand{\GammaFluid}{\Gamma_{F}}
\newcommand{\GammaSolid}{\Gamma_{S}}
\newcommand{\BoSSS}{\textit{BoSSS}}


\begin{document}


\chapter{Parallel Performance and Scaling}
This section covers basic performance tests, i.e. how specific algorithms scale in parallel with increasing \emph{number of processors}. So far, all calculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt.

\section{ Xdg Poisson, steady droplet }
\subsection{weak scaling}
Weak scaling is investigated of the test problem introduced in \ref{sec:XdgPoisson}. This means the problem size per processor is constant during the study (in this case approximately 10.000 DOF / core). So the total problem size grows with increasing number of cores. The expectation is that wall clock time remains constant trough the study. This means in the same time frame with $N$ cores we are able to solve a system, which is $N$ times the size of a single core run.

In \ref{weakXdgPoissonScaling} the scaling of V-krylov-cycle is shown:

\graphicspath{{./apdx-MPISolverPerformance/weakScaling/XdgPoisson/plots/}} 
\begin{figure}[h!]
	\begin{center}
		\input{./apdx-MPISolverPerformance/weakScaling/XdgPoisson/plots/Scaling_2.tex}
	\end{center}
	\caption{
		Solver wall clock time vs. no of processors, for polynomial degree $k=2$ and approximately 10.000 DOF / processor, Grid partitioning with METIS (except 128 cores with predefined partitioning),
		for problem/Equation (\ref{eq:poisson-jump-problem-def}).
	}
	\label{fig:weakXdgPoissonScaling}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\input{./apdx-MPISolverPerformance/weakScaling/XdgPoisson/plots/Profiling_2.tex}
	\end{center}
	\caption{
		profiling of the V-kcycle run for the same setting:
		for problem/Equation (\ref{eq:poisson-jump-problem-def}).
	}
	\label{fig:weakXdgPoisson-kcycle-profiling}
\end{figure}

\subsection{strong scaling}

%\graphicspath{{./apdx-MPISolverPerformance/strongScaling/XdgPoisson/plots/}} 
%\begin{figure}[h!]
%	\begin{center}
%		\input{./apdx-MPISolverPerformance/strongScaling/XdgPoisson/plots/Scaling_2.tex}
%	\end{center}
%	\caption{
%		Solver wall clock time vs. no of processors, for polynomial degree $k=2$ and approximately 10.000 DOF / processor, Grid partitioning with METIS (except 128 cores with predefined partitioning),
%		for problem/Equation (\ref{eq:poisson-jump-problem-def}).
%	}
%	\label{fig:weakXdgPoissonScaling}
%\end{figure}

%\begin{figure}[h!]
%	\begin{center}
%		\input{./apdx-MPISolverPerformance/strongScaling/XdgPoisson/plots/Profiling_2.tex}
%	\end{center}
%	\caption{
%		profiling of the V-kcycle run for the same setting:
%		for problem/Equation (\ref{eq:poisson-jump-problem-def}).
%	}
%	\label{fig:weakXdgPoisson-kcycle-profiling}
%\end{figure}


\section{Stokes rotating sphere}
In this section we will stick to a rigid body $\Omega_S$ which is embedded in the computational domain $\Omega \subset \mathbb{R}^D$, and the fluid domain is given as 
$\Omega_F(t)=\Omega \setminus \overline{\Omega}_S(t)$.
We further assume the incompressible Navier-Stokes equation:



\begin{equation}
\left\{ \begin{array} {rclll}
\frac{\partial \rho \vec{u}}{\partial t}+ \nabla \cdot ( \rho \vec{u} \otimes \vec{u}) + \nabla p - \eta \Delta \vec{u} & = & \vec{f} \quad & in \ \ \Omega_F(t) \times (0,T) \\
\nabla \cdot \vec{u} & = & 0 \quad & in \ \ \Omega_F(t) \times (0,T) \\
\vec{u}(\vec{x},0) & = & \vec{0} \quad & on \ \ \Omega_F(0) \\
p(\vec{x},0) & = & 0 \quad & on \ \ \Omega_F(0) \\
\vec{u}(\vec{x},t) & = & \vec{0} \quad & on \ \ \partial\Omega_F \setminus \Interface \\
\vec{u}(\vec{x},t) & = & \boldsymbol{\omega}(t) \times \vec{r} \quad & on \ \ \Interface = \partial\Omega_S \cap \partial\Omega_F 
\end{array} \right.
\end{equation}
where rigid body $\Omega_S(t)$ is rotating with the angular velocity of $\boldsymbol{\omega}$. At the domain boundary $\partial\Omega_F \setminus \Interface$ and at the interface dirichlet boundary conditions are imposed. The surface $\partial \Omega_S$, respectively $\Interface$, is represented by the isocontour of the level-set $\varphi(x,t)$.

A sphere is defined by the isocontour of: 
\begin{equation}
	-x_1^2-x_2^2-x_3^2+r^2=0.
\end{equation}

\subsection{parallel efficiency (weak scaling)}
\begin{figure}[H]
	\centering
	\pgfplotsset{xlabel = cores, ylabel= Efficiency $T(8)/T(P)$, width = 0.6\textwidth, height= 0.45\textwidth}
	\begin{tikzpicture}
	\begin{loglogaxis}[
	xmin=8 ,xmax=256,
	legend columns=-1,
	%legend to name=bastingComparisonLegend,
	ymin=0,ymax=1,
	xtick={8,16,32,64,128,256},
	xticklabels={$\mathsf{8}$, $\mathsf{16}$, $\mathsf{32}$, $\mathsf{64}$, $\mathsf{128}$, $\mathsf{256}$}, 
	legend style={at={(.5,1.1)},anchor=north},
	]
	\def \WeakPath {./apdx-MPISolverPerformance/weakScaling/DG_rotSphere/plots}
	
	\addplot[mark=none, red] table[y=Speedup] {\WeakPath/weak_k2.dat};
	\addplot[mark=none, blue] table[y=Speedup] {\WeakPath/weak_k3.dat};
	\addplot[mark=none, orange] table[y=Speedup] {\WeakPath/weak_k4.dat};
	\legend{ $k2$, $k3$, $k4$ };
	\end{loglogaxis}
	\end{tikzpicture}
	
	\caption{weak scaling efficiency, weak scaling: constant degrees of freedom per core, sweep from 8 to 256 cores }
	\label{plt:weak_scaling}
\end{figure}

\subsection{parallel speedup (strong scaling)}
\begin{figure}[H]
	\centering
	\pgfplotsset{xlabel = cores, ylabel= Speedup $T(8)/T(P)$, width = 0.6\textwidth, height= 0.45\textwidth}
	\begin{tikzpicture}
	\begin{loglogaxis}[
	xmin=8 ,xmax=256,
	legend columns=-1,
	%legend to name=bastingComparisonLegend,
	ymin=1,ymax=32,
	xtick={8,16,32,64,128,256},
	xticklabels={$\mathsf{8}$, $\mathsf{16}$, $\mathsf{32}$, $\mathsf{64}$, $\mathsf{128}$, $\mathsf{256}$}, 
	legend style={at={(.5,1.1)},anchor=north},
	]
	\def \StrongPath {./apdx-MPISolverPerformance/strongScaling/DG_rotSphere/plots}
	
	\addplot[mark=none, red] table[y=Speedup] {\StrongPath/strong_k2.dat};
	\addplot[mark=none, blue] table[y=Speedup] {\StrongPath/strong_k3.dat};
	\addplot[mark=none, orange] table[y=Speedup] {\StrongPath/strong_k4.dat};
	\legend{ $k2$, $k3$, $k4$ };
	\end{loglogaxis}
	\end{tikzpicture}
	
	\caption{maximum runtime per linear solver iteration over all cores, sweep from 8 to 256 cores, }
	\label{plt:strong_scaling}
\end{figure}

\subsection{profile}
\begin{figure}[H]
	\centering
	\pgfplotsset{xlabel = cores, ylabel= runtime per iteration [sec] , width = 0.6\textwidth, height= 0.45\textwidth}
	\begin{tikzpicture}
	\begin{axis}
	[ybar stacked, enlargelimits=0.25,
	symbolic x coords={8, 16, 32, 64, 128, 256},
	%nodes near coords,
	%nodes near coords xbar stacked configuration/.style={},
	xtick=data,
	%every node near coord/.append style={xshift=5pt},
	legend style={at={(1.1,.5)},anchor=west},
	%totals/.style={nodes near coords align={anchor=south}},
	%x tick label style={anchor=south,yshift=-0.5cm},
	]
	%MatrixAssembly_time	AggregationBaseInit_time	DataIO_time	CGProjection_time	SayeCompile_time	StandardCompile_time	AMR_time	LoadBal_time	SolverInit_time	SolverRun_time
	
	\def \ProfilePath {./apdx-MPISolverPerformance/strongScaling/DG_rotSphere/plots/}
	
	
	\addplot table[y=AggregationBaseInit_time] {\ProfilePath};
	\addplot table[y=DataIO_time] {\ProfilePath};
	%\addplot table[y=CGProjection_time] {\ProfilePath};
	\addplot table[y=SayeCompile_time] {\ProfilePath};
	\addplot table[y=StandardCompile_time] {\ProfilePath};
	%\addplot table[y=AMR_time] {\ProfilePath};
	\addplot table[y=LoadBal_time] {\ProfilePath};
	\addplot table[y=SolverInit_time] {\ProfilePath};
	\addplot table[y=MatrixAssembly_time] {\ProfilePath};
	\addplot table[y=SolverRun_time,nodes near coords, nodes near coords align={horizontal}] {\ProfilePath};
	
	%\legend{ $MatrixAssembly_time$, $AggregationBaseInit_time$, $DataIO_time$, $CGProjection_time$, $SayeCompile_time$, $StandardCompile_time$, $AMR_time$, $LoadBal_time$, $SolverInit_time$, $SolverRun_time$ };
	\legend{  $AggregationBaseInit_time$, $DataIO_time$, $SayeCompile_time$, $StandardCompile_time$, $LoadBal_time$, $SolverInit_time$, $MatrixAssembly_time$, $SolverRun_time$ };
	\end{axis}
	\end{tikzpicture}
	\caption{k4, runtime profile of IBM solver}
	\label{plt:profiling}
\end{figure}

\end{document}